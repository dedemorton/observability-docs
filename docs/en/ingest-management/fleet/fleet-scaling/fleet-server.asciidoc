[[fleet-server]]
= {fleet-server}

{fleet-server} is a component of the {stack} used to centrally manage {agent}s.
It's launched as part of an {agent} on a host intended to act as a server.
One {fleet-server} process can support many {agent} connections,
and serves as a control plane for updating agent policies, collecting
status information, and coordinating actions across {agent}s.

{fleet-server} is the mechanism {agent}s use to communicate with {es}:

. When a new agent policy is created, it's saved to {es}.

. To enroll in the policy, {agent}s send a request to {fleet-server},
using the enrollment key generated for authentication.

. {fleet-server} receives the request and gets the agent policy from {es},
then ships the policy to all {agent}s enrolled in that policy.

. {agent} uses configuration information in the policy to collect and send data
to {es}.

. {fleet-server} periodically checks {agent}s for status information.

. When a policy is updated, {fleet-server} retrieves the updated policy from
{es} and sends it to the connected {agent}s.

image::fleet/images/fleet-server-communication.png[{fleet-server} handles communication between {agent}, {fleet}, {es}, and {kib}]

{fleet-server} runs as a subprocess inside an {agent}. The agent uses a special
policy that describes the {fleet-server} configuration. In large scale
self-managed deployments or on {ess-product}[hosted {ess}] on {ecloud},
{fleet-server} is typically run as a dedicated {agent} communication host, but
you can optionally use it for data collection on self-managed clusters. For more
details, refer to <<fleet-server-deployment>>.

[discrete]
[[deployment-models]]
== {fleet-server} deployment models

Administrators deploying the {agent} have a few deployment choices
available to satisfy their organization's requirements. {fleet-server} can be
deployed:

* On-prem and self-managed, or
* On {ecloud}, as part of our hosted {ess}, which is managed by Elastic.


[discrete]
[[deployed-in-cloud]]
=== Deployed in {ecloud}

To simplify the deployment of {agent}, the {fleet-server} can be
provisioned and hosted in the {ecloud}. In this case, when the deployment is
created, a highly available set of {fleet-server}s are automatically deployed.

Administrators can choose the resources allocated to the {fleet-server} and
whether they want the {fleet-server} to be deployed in multiple availability
zones. (Cloud → Deployments → _deployment name_ → Edit).

* Modify the compute resources available to the server to accommodate higher
scale of {agent}s
* Modify the availability zones to satisfy fault tolerance requirements

[role="screenshot"]
image::images/fleet-server-hosted-container.png[{fleet-server} hosted agent]

Once deployed on {ecloud} as a service, the full life cycle of the
{fleet-server} is managed by Elastic. {fleet-server} is scalable and highly
available with traffic ingress load balanced across multiple instances to
satisfy the scale requirements.

image::images/fleet-server-cloud-deployment.png[{fleet-server} Cloud deployment model]

[discrete]
[[deployed-on-prem]]
=== Deployed on-prem

{fleet-server} can be deployed on-premises and managed by the user. In this
deployment model, the administrator is responsible for {fleet-server} deployment
and lifecycle management . This mode of operation is predominantly chosen to
satisfy data governance requirements or used in scenarios where the agents only
have access to a private segmented network.

It’s recommended that the administrator provision multiple instances of the
{fleet-server} and use a load balancer to better scale the deployment.

image::images/fleet-server-on-prem-deployment.png[{fleet-server} on-prem deployment model]

[discrete]
[[fleet-server-HA-operations]]
== {fleet-server} High availability operations

{fleet-server} is stateless. Connections to the {fleet-server} therefore can be
load balanced as long as the {fleet-server} has capacity to accept more
connections. As mentioned earlier load balancing is done on a round-robin basis.

In the {ecloud} deployment model, multiple {fleet-server}s will automatically be
provisioned to satisfy the instance size chosen (instance sizes are modified to
satisfy the scale requirement). In addition, if the user choses multiple
availability zones to address their fault-tolerance requirements, those
instances will be utilized to balance the load also.

In an on-prem deployment, high-availability, fault-tolerance, and lifecycle
management of the {fleet-server} are the responsibility of the administrator.

[discrete]
[[fleet-server-scalability]]
== {fleet-server} scalability

This section summarizes the resource and {fleet-server} configuration
requirements needed to scale your deployment of {agent}s.

The {fleet-server} configuration is applied through an agent policy. To
modify the policy, navigate to the {ecloud} agent policy and choose *Edit
Integration* (Fleet → Agent Policies → {ecloud} Agent Policy → Edit
Integration).

[role="screenshot"]
image::images/elastic-cloud-agent-policy.png[{ecloud} policy]

Under the {fleet-server} advanced options, modify specific {fleet-server}
parameters including *Max Connections*.

For scaling requirements and recommendations, refer to <<scaling-recommendations>>.

[role="screenshot"]
image::images/fleet-server-configuration.png[{fleet-server} configuration]

[discrete]
[[fleet-server-configuration]]
=== Advanced {fleet-server} configuration settings

The following advanced settings are available to fine tune your {fleet-server}
deployment.

//TODO: We need to discuss the best way to format config settings. I'm using
//mostly tables in other sections of the docs for improved scanning, but that is
//controversial.

//Nima: I tried removed repetitive words and tried clarify the descriptions here,
//but there were some inconsistencies. You'll want to check this carefully.

`cache`::

`num_counters`:::
Size of the hash table. Best practice is to have this set to 10x max
connections.

`max_cost`:::
Total size of the cache.

`server.limits`::
`policy_throttle`:::
How often a new policy is rolled out to the agents.

`checkin_limit.interval`:::
How fast the agents can check in to the {fleet-server}.

`checkin_limit.burst`:::
Burst of check-ins allowed before falling back to the rate defined by
`interval`.

`checkin_limit.max`:::
Maximum number of agents.

`artifact_limit.max`:::
Maximum number of agents that can call the artifact API concurrently. It allows
the user to avoid overloading the {fleet-server} from artifact API calls.

`artifact_limit.interval`:::
How often artifacts are rolled out. Default of 100ms allows 10 artifacts to be
rolled out per second.

`artifact_limit.burst`:::
Number of transactions allowed for a burst, controlling oversubscription on
outbound buffer.

`ack_limit.max`:::
Maximum number of agents that can call the Ack API concurrently. It allows the
user to avoid overloading the {fleet-server} from Ack API calls.

`ack_limit.interval`:::
How often an acknowledgment (ACK) is sent. Default value of 10ms enables 100
ACKs per second to be sent.

`ack_limit.burst`:::
Burst of ACKs to accommodate (default of 20) before falling back to the rate
defined in `interval`.

`enroll_limit.max`:::
Maximum number of agents that can call the Enroll API concurrently. This setting
allows the user to avoid overloading the {fleet-server} from Enrollment API
calls.

`enroll_limit.interval`:::
Interval between processing enrollment request. Enrollment is both CPU and RAM
intensive, so the number of enrollment requests needs to be limited for overall
system health. Default value of 100ms allows 10 enrollments per second.

`enroll_limit.burst`:::
Burst of enrollments to accept before falling back to the rate defined by
`interval`.

[discrete]
[[scaling-recommendations]]
=== Scaling recommendations

The following tables provide system requirements and scaling guidelines based
on the number of agents required by your deployment:

* <<system-requirements-by-number-agents>>
* <<recommend-settings-scaling-agents-a>>
* <<recommend-settings-scaling-agents-b>>

//Are these guidelines all cloud-specific?

[discrete]
[[system-requirements-by-number-agents]]
==== System requirements by number of agents
|===
| Number of Agents | Memory      | vCPU           | {es} Cluster size

| 50               | 512MB    .5+| Up to 2.5 vCPU | 480GB disk \| 16GB RAM \| up to 5 vCPU
| 5,000            | 1GB                          | 960GB disk \| 32GB RAM \| 5 vCPU
| 7,500            | 2GB                          | 1.88TB disk \| 64GB RAM \| 9.8 vCPU
| 10,000           | 4GB                          | 3.75TB disk \| 128GB RAM \| 19.8 vCPU
| 12,500           | 8G                           | 7.5TB disk \| 256GB RAM \| 39.4 vCPU
| 30,000           | 16GB     .2+| 2.5 vCPU       | 7.5TB disk \| 256GB RAM \| 39.4 vCPU
| 50,000           | 32GB                         | 11.25TB disk \| 384GB RAM \|59.2 vCPU
|===

[discrete]
[[recommend-settings-scaling-agents-a]]
==== Recommended settings for 50 to 10,000 agents
|===
|                      | *50*    | *5,000*  | *7,500*  | *10,000*
| *Max connections*    | 100     | 7,000    | 10,000   | 20,000
5+s| Cache settings
| `num_counters`      | 2000    | 20000    | 40000    | 80000
| `max_cost`          | 2097152 | 20971520 | 50971520 | 104857600
5+s| Server limits
| `policy_throttle`   | 200ms   | 50ms     | 10ms     | 5ms
5+| `checkin_limit:`
>| `interval`          | 50ms    | 5ms      | 2ms      | 1ms
>| `burst`             | 25      | 500      | 1000     | 2000
>| `max`               | 100     | 5001     | 7501     | 10001
5+| `artifact_limit:`
>| `interval`          | 100ms   | 5ms      | 2ms      | 1ms
>| `burst`             | 10      | 500      | 1000     | 2000
>| `max`               |1 0      | 1000     | 2000     | 4000
5+| `ack_limit:`
>| `interval`          | 10ms    | 4ms      | 2ms      | 1ms
>| `burst`             | 20      | 500      | 1000     | 2000
>| `max`               | 20      | 1000     | 2000     | 4000
5+| `enroll_limit:`
>| `interval`          | 100ms   | 20ms     | 10ms     | 10ms
>| `burst`             | 5       | 50       | 100      | 100
>| `max`               | 10      | 100      | 200      | 200
5+s| Server runtime settings
| `gc_percent`         | 20      | 20       | 20       | 20
|===

[discrete]
[[recommend-settings-scaling-agents-b]]
==== Recommended settings for 12,500 to 50,000 agents

|===
|                      | *12,500*  | *30,000*  | *50,000*
| *Max connections*    | 32,000    | 32,000    | 32,000
4+s| Cache settings
| `num_counters`       | 160000    | 160000    | 320000
| `max_cost`           | 209715200 | 209715200 | 209715200
4+s| Server limits
| `policy_throttle`    | 5ms       | 2ms       | 5ms
4+| `checkin_limit:`
>| `interval`          | 500us     | 500us     | 500us
>| `burst`             | 4000      | 4000      | 4000
>| `max`               | 12501     | 15001     | 25001
4+| `artifact_limit:`
>| `interval`          | 500us     | 500us     | 500us
>| `burst`             | 4000      | 4000      | 4000
>| `max`               | 8000      | 8000      | 8000
4+| `ack_limit:`
>| `interval`          | 500us     | 500us     | 500us
>| `burst`             | 4000      | 4000      | 4000
>| `max`               | 8000      | 8000      | 8000
4+| `enroll_limit:`
>| `interval`          | 10ms      | 10ms      | 10ms
>| `burst`             | 100       | 100       | 100 
>| `max`               | 200       | 200       | 200
4+s| Server runtime settings
| `gc_percent`         | 20        | 20        | 20
|===

[discrete]
[[fleet-server-monitoring]]
== {fleet-server} monitoring

//Suggestion: Talk about the metrics and logs that {fleet-server} provides, how
//users can enable and see them, how to use them to determine when to scale up
//{fleet-server}.

Monitoring {fleet-server} is key since the operation of the {fleet-server} is
paramount to the health of the deployed agents and the services they offer. When
{fleet-server} is not operating correctly, it may lead to delayed check-ins,
status information, and updates for the agents it manages. The monitoring data
will tell you when to add capacity for {fleet-server}, and provide error logs
and information to troubleshoot other issues.

To enable monitoring for {fleet-server}, you must enable agent monitoring in the
agent policy. It is enabled by default when you create a new agent policy and in
the Default {fleet-server} agent policy in self-managed clusters. However, it is
disabled by default in {ecloud} agent policy because enabling monitoring will
require additional RAM.

To modify {ecloud} agent policy, navigate to the _{ecloud} agent Policy_*_
(_*_Fleet → Agent Policies → {ecloud} Agent Policy_).

[role="screenshot"]
image::images/fleet-policy-page.png[Fleet Policy Page]

Choose the *Settings* tab for the _{ecloud} agent policy_. Agent Monitoring is
disabled by default. Once enabled the agent will be able to collect logs and
metrics from the {fleet-server}.

NOTE: The {fleet-server} is deployed as yet another agent in the system.

[role="screenshot"]
image::images/elastic-cloud-agent-policy-page.png[{ecloud} Policy Page]

In many scenarios it’s desirable to segregate the {fleet-server} monitoring data
from other agents’ data. To do this the user has the ability to define a
*Default namespace* to make it easier to search and visualize the monitoring
data. By default the monitoring data is sent to the *default* namespace. In
the following example, {fleet-server} was configured as the namespace, and you
can see the metrics collected:

[role="screenshot"]
image::images/dashboard-with-namespace-showing.png[Namespace]

[role="screenshot"]
image::images/datastream-namespace.png[Datastream]

A predefined dashboard called *[{agent}] Agent metrics* is loaded into {kib}.
Choose this dashboard and query based on the namespace defined for the
{fleet-server}. The following dashboard shows data for the query
`_data_stream.namespace: "fleetserver_"`. In this example, you can observe CPU
and memory usage as a metric and act accordingly to resize the {fleet-server}.

[role="screenshot"]
image::images/dashboard-datastream.png[Dashboard Datastream]
