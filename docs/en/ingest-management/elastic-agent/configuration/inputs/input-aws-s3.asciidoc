:input-type: aws-s3

[[aws-s3-input]]
= AWS S3 input

++++
<titleabbrev>AWS S3</titleabbrev>
++++

beta[]

Use the `aws-s3` input to retrieve logs from S3 objects that are pointed by
messages from specific SQS queues. This input can, for example, be used to
receive S3 server access logs to monitor detailed records for the requests that
are made to a bucket.

When processing a s3 object which pointed by a sqs message, if half of the set
visibility timeout passed and the processing is still ongoing, then the
visibility timeout of that sqs message will be reset to make sure the message
does not go back to the queue in the middle of the processing. If there are
errors happening during the processing of the s3 object, then the process will
be stopped and the sqs message will be returned back to the queue.

[source,yaml]
----
inputs:
  - type: aws-s3
    queue_url: https://sqs.ap-southeast-1.amazonaws.com/1234/test-s3-queue
    credential_profile_name: elastic-agent
    expand_event_list_from_field: Records
----

[[input-aws-s3-credentials]]
== Required AWS credentials and permissions

To make AWS API calls, the `aws-s3` input requires AWS credentials.

//See <<aws-credentials-config,AWS credentials options>> for more details.
//TODO: need to add the credential config info to the agent docs.

To access `aws-cloudwatch`, the IAM user must have the required permissions to
access SQS and S3:

----
s3:GetObject
sqs:ReceiveMessage
sqs:ChangeMessageVisibility
sqs:DeleteMessage
----

[[input-aws-s3-sqs-setup]]
== S3 and SQS setup

To set up S3, enable bucket notification. Any new object creation in the S3
bucket will also create a notification through SQS.

For more information, see the Amazon S3 documentation about
https://docs.aws.amazon.com/AmazonS3/latest/dev/ways-to-add-notification-config-to-bucket.html[configuring a bucket for notifications].

//QUESTION: Do we want to include the section about parallel processing? It
//talks about running multiple instances of Filebeat, so I'm not sure whether
//it's relevant in the brave new world of Elastic Agent.

[[input-aws-s3-configuration-settings]]
== Configuration settings

The `aws-s3` input supports the following settings. Many of these settings have
sensible defaults that allow you to run {agent} with minimal configuration.

{also-see-common-input-settings}

[id="input-aws-s3-api_timeout-setting"]
`api_timeout`::
(string) The maximum duration an AWS API can take. If the API exceeds the
timeout, it is interrupted. The minimum is 0 seconds. The maximum is half of the
visibility timeout value.
+
Default: `120s`

[id="input-aws-s3-expand_event_list_from_field-setting"]
`expand_event_list_from_field`::
(string) The name of a field that bundles multiple messages. Use this setting
for filesets that receive multiple messages bundled under a specific field.
For example, CloudTrail logs are in JSON format, and events are found under the
JSON object "Records".
+
[source,json]
----
{
    "Records": [
        {
            "eventVersion": "1.07",
            "eventTime": "2019-11-14T00:51:00Z",
            "awsRegion": "us-east-1",
            "eventID": "EXAMPLE8-9621-4d00-b913-beca2EXAMPLE",
        },
        {
            "eventVersion": "1.07",
            "eventTime": "2019-11-14T00:52:00Z",
            "awsRegion": "us-east-1",
            "eventID": "EXAMPLEc-28be-486c-8928-49ce6EXAMPLE",
        }
    ]
}
----
+
This setting splits the messages under the group value (`"Records"`) into
separate events.
+
NOTE: When this setting is specified, the `aws-s3` input assumes logs are in
JSON format and decodes them as JSON. The content type is not checked. If a
file has an "application/json" content-type, the `expand_event_list_from_field`
setting is required to read the JSON file.

//QUESTION: This field appears in the Kafka input, too. We can use a shared
//description, but we would need to make the example more generic so that it
//works for both. WDYT?

[id="input-aws-s3-file_selectors-setting"]
`file_selectors`::
(list) If the SQS queue has events that correspond to files {agent} shouldn't
process, set `file_selectors` to limit the files that are downloaded. Each list
of selectors has `regex` and `expand_event_list_from_field` settings.  The
`regex` should match the S3 object key in the SQS message, and the optional
`expand_event_list_from_field` is the same as the global setting. If
`file_selectors` is given, then any global `expand_event_list_from_field` value
is ignored in favor of the ones specified in the `file_selectors`. Regex syntax
is the same as the Go language.  Files that don't match one of the regexes won't
be processed.
+
[source,yaml]
----
file_selectors:
  - regex: '^AWSLogs/\d+/CloudTrail/'
    expand_event_list_from_field: 'Records'
  - regex: '^AWSLogs/\d+/CloudTrail-Digest'
----

[id="input-aws-s3-fips_enabled-setting"]
`fips_enabled`::
(boolean) If `true`, changes the service name from `s3` to `s3-fips` for
connecting to the correct service endpoint. For example:
`s3-fips.us-gov-east-1.amazonaws.com`.
+
Default: `false`

//QUESTION: Is this default correct? Filebeat docs didn't say.

[id="input-aws-s3-max_number_of_messages-setting"]
`max_number_of_messages`::
(int) The maximum number of messages to return. Amazon SQS never returns more
messages than this value. However, fewer messages may be returned. Valid values:
1 to 10. 
+
Default: `5`

[id="input-aws-s3-queue_url-setting"]
`queue_url`::
(string) URL of the AWS SQS queue that messages will be received from. This
setting is required.

[id="input-aws-s3-visibility_timeout-setting"]
`visibility_timeout`::
+
(string) The duration that the received messages are hidden from subsequent
retrieve requests after being retrieved by a ReceiveMessage request. This value
needs to be a lot bigger than the {agent} collection frequency. If it takes too
long to read the s3 log, the sqs message will not be reprocessed. The minimum
visibility timeout is 0 seconds. The maximum is 12 hours.
+
Default: `300s`

//QUESTION: Is this the correct format for visiblity timout? Not sure because
//the docs don't say.
