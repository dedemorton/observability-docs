// These settings are shared across the docs for multiple inputs. Copy and use
// the following template to add a shared setting. Replace values in all caps.
// Use an include statement // to pull the tagged region into your source file:
// include::input-shared-settings.asciidoc[tag=NAME-setting]

// tag::NAME-setting[]
|
[id="input-{input-type}-NAME-setting"]
`NAME`

| (TYPE) DESCRIPTION.

*Default:* `DEFAULT`

OPTIONAL INFO AND EXAMPLE
// end::NAME-setting[]

// =============================================================================

//TODO: Convert examples to correct syntax for Elastic Agent

// tag::backoff-setting[]
|
[id="input-{input-type}-backoff-setting"]
`backoff`

| (string) Specifies how long {agent} waits before checking a file again after
EOF is reached. Use this setting to control how aggressively {agent} crawls open
files for updates. You can use the default values in most cases.

*Default:* `1s` (the file is checked every second if new lines were added)

The default enables near real-time crawling. Every time a new line appears in
the file, the `backoff` value is reset to the initial value.
// end::backoff-setting[]

// =============================================================================

// tag::backoff_factor-setting[]
|
[id="input-{input-type}-backoff_factor-setting"]
`backoff_factor-setting`

| (int) Specifies how fast the waiting time is increased. The bigger the
backoff factor, the faster the `max_backoff` value is reached. The backoff
factor increments exponentially. The minimum value allowed is `1`. If this value
is set to `1`, the backoff algorithm is disabled, and the `backoff` value is used
for waiting for new lines. The `backoff` value will be multiplied each time with
the `backoff_factor` until `max_backoff` is reached.

*Default:* `2`
// end::backoff_factor-setting[]

// =============================================================================

// tag::clean-setting[]
|
[id="input-{input-type}-clean_*-setting"]
`clean_*`

| The `clean_*` options are used to clean up the state entries in the registry
file. These settings help to reduce the size of the registry file and can
prevent a potential inode reuse issue.

// end::clean-setting[]

// =============================================================================

// tag::clean_inactive-setting[]
|
[id="input-{input-type}-clean_inactive-setting"]
`clean_inactive`

|
WARNING: Only use this setting if you understand that data loss is a potential
side effect.

(string) When this option is enabled, {agent} removes the state of a file after
the specified period of inactivity has elapsed. The state can only be removed
if the file is already ignored by {agent} (the file is older than
`ignore_older`). The `clean_inactive` setting must be greater than `ignore_older
+ scan_frequency` to make sure that no states are removed while a file is still
being harvested. Otherwise, the setting could result in {agent} resending the
full content constantly because `clean_inactive` removes state for files that
are still detected by {agent}. If a file is updated or appears again, the file
is read from the beginning.

*Default:* `0` (disabled)

The `clean_inactive` configuration setting is useful to reduce the size of the
registry file, especially if a large amount of new files are generated every
day.

This config setting is also useful to prevent {agent} problems resulting
from inode reuse on Linux.

NOTE: Every time a file is renamed, the file state is updated and the counter
for `clean_inactive` starts at 0 again.

TIP: During testing, you might notice that the registry contains state entries
that should be removed based on the `clean_inactive` setting. This happens
because {agent} doesn't remove the entries until it opens the registry
again to read a different file. If you are testing the `clean_inactive` setting,
make sure {agent} is configured to read from more than one file, or the
file state will never be removed from the registry. 

// end::clean_inactive-setting[]

// =============================================================================

// tag::clean_removed-setting[]
|
[id="input-{input-type}-clean_removed-setting"]
`clean_removed`

| (boolean) When `true`, {agent} cleans files from the registry if they cannot be
found on disk anymore under the last known name. This also means that files
renamed after the harvester finished are removed. 

If a shared drive disappears for a short period and appears again, all files
are read again from the beginning because the states were removed from the
registry file. In such cases, we recommend that you set `clean_removed` to
`false`.

If `close_removed` is `false`, you must set `clean_removed` to `false`, too.

*Default:* `true`
// end::clean_removed-setting[]

// =============================================================================

// tag::close-setting[]
|
[id="input-{input-type}-close-setting"]
`close_*`

| The `close_*` settings are used to close the harvester after a
certain criteria or time. Closing the harvester means closing the file handler.
If a file is updated after the harvester is closed, the file will be picked up
again after `scan_frequency` has elapsed. However, if the file is moved or
deleted while the harvester is closed, {agent} will not be able to pick up
the file again, and any data that the harvester hasn't read will be lost. 

The `close_*` settings are applied synchronously when {agent} attempts 
to read from a file, meaning that if {agent} is in a blocked state
due to blocked output, full queue or other issue, a file that would 
otherwise be closed remains open until {agent} once again attempts to read from the file.

// end::close-setting[]

// =============================================================================

// tag::close_eof-setting[]
|
[id="input-{input-type}-close_eof-setting"]
`close_eof`

| 
WARNING: Only use this option if you understand that data loss is a potential
side effect.

(boolean) When `true`, {agent} closes a file as soon as the end of a
file is reached. This is useful when your files are only written once and not
updated from time to time. For example, this happens when you are writing every
single log event to a new file.

*Default:* `false`
// end::close_eof-setting[]

// =============================================================================

// tag::close_inactive-setting[]
|
[id="input-{input-type}-close_inactive-setting"]
`close_inactive`

| (string) If specified, {agent} closes the file handle if the file has not been
harvested for the specified duration. The counter for the defined period starts
when the last log line was read by the harvester. It is not based on the
modification time of the file. If the closed file changes again, a new harvester
is started, and the latest changes will be picked up after `scan_frequency` has
elapsed.

*Default:* `5m`

We recommended that you set `close_inactive` to a value that is larger than the
least frequent updates to your log files. For example, if your log files get
updated every few seconds, you can safely set `close_inactive` to `1m`. If there
are log files with very different update rates, you can use multiple
configurations with different values.

Setting `close_inactive` to a lower value means that file handles are closed
sooner. However this has the side effect that new log lines are not sent in near
real time if the harvester is closed.

The timestamp for closing a file does not depend on the modification time of the
file. Instead, {agent} uses an internal timestamp that reflects when the
file was last harvested. For example, if `close_inactive` is set to 5 minutes,
the countdown for the 5 minutes starts after the harvester reads the last line
of the file.

You can use time strings like 2h (2 hours) and 5m (5 minutes).

// end::close_inactive-setting[]

// =============================================================================

// tag::close_renamed-setting[]
|
[id="input-{input-type}-close_renamed-setting"]
`close_renamed`

|
WARNING: Only use this option if you understand that data loss is a potential
side effect.

(boolean) When `true`, {agent} closes the file handler when a file
is renamed. This happens, for example, when rotating files.

*Default:* `false` (the harvester stays open and keeps reading the file because
it does not depend on the file name)

If `close_renamed` is `true` and the file is renamed or moved in such a way that
it's no longer matched by the file patterns specified for the path, the file
will not be picked up again. {agent} will not finish reading the file.

Do not use this option when `path` based `file_identity` is configured. It does
not make sense to enable the option, because {agent} cannot detect renames using
path names as unique identifiers.

TIP: If your Windows log rotation system shows errors because it can't rotate
files, set this option to `true`.

// end::close_renamed-setting[]

// =============================================================================

// tag::close_removed-setting[]
|
[id="input-{input-type}-close_removed-setting"]
`close_removed`

| (boolean) When `true`, {agent} closes the harvester when a file is
removed.

*Default:* `true`

Normally a file should only be removed after it's inactive for the duration
specified by `close_inactive`. However, if a file is removed early and
`close_removed` is `false`, {agent} keeps the file open to make sure the
harvester has completed. If this setting results in files that are not
completely read because they are removed from disk too early, set
`close_removed` to `false`.

If `close_removed` is `false`, you must also set `clean_removed` to `false`.


TIP: If your Windows log rotation system shows errors because it can't rotate
files, set this option to `true`.


// end::close_removed-setting[]

// =============================================================================

// tag::close_timeout-setting[]
|
[id="input-{input-type}-close_timeout-setting"]
`close_timeout`

|
WARNING: Only change this setting if you understand that data loss is a
potential side effect. Another side effect is that multiline events might not be
completely sent before the timeout expires.

(string) When this setting is enabled, {agent} gives every harvester a predefined
lifetime. Regardless of where the reader is in the file, reading will stop after
the `close_timeout` period has elapsed.

*Default:* `0` (disabled)

This setting can be useful for older log files when you want to spend only a
predefined amount of time on the files. While `close_timeout` will close the
file after the predefined timeout, if the file is still being updated, {agent}
will start a new harvester again per the defined `scan_frequency`. And the
`close_timeout` for this harvester will start again with the countdown for the
timeout.

This option is particularly useful in case the output is blocked, which makes
{agent} keep open file handlers even for files that were deleted from the
disk. Setting `close_timeout` to `5m` ensures that the files are periodically
closed so they can be freed up by the operating system.

If you set `close_timeout` to equal `ignore_older`, the file will not be picked
up if it's modified while the harvester is closed. This combination of settings
normally leads to data loss, and the complete file is not sent.

When you use `close_timeout` for logs that contain multiline events, the
harvester might stop in the middle of a multiline event, which means that only
parts of the event will be sent. If the harvester is started again and the file
still exists, only the second part of the event will be sent.
// end::close_timeout-setting[]

// =============================================================================


// tag::enabled-setting[]
|
[id="input-{input-type}-enabled-setting"]
`enabled`

| (boolean) Enable or disable the input.

*Default:* `true`
// end::enabled-setting[]

// =============================================================================

// tag::encoding-setting[]
|
[id="input-{input-type}-encoding-setting"]
`encoding`

| (string) The file encoding to use for reading data that contains international
characters. See the encoding names http://www.w3.org/TR/encoding/[recommended by
the W3C for use in HTML5]. Valid encodings:

	* `plain`: plain ASCII encoding
	* `utf-8` or `utf8`: UTF-8 encoding
	* `gbk`: simplified Chinese charaters
	* `iso8859-6e`: ISO8859-6E, Latin/Arabic
	* `iso8859-6i`: ISO8859-6I, Latin/Arabic
	* `iso8859-8e`: ISO8859-8E, Latin/Hebrew
	* `iso8859-8i`: ISO8859-8I, Latin/Hebrew
	* `iso8859-1`: ISO8859-1, Latin-1
	* `iso8859-2`: ISO8859-2, Latin-2
	* `iso8859-3`: ISO8859-3, Latin-3
	* `iso8859-4`: ISO8859-4, Latin-4
	* `iso8859-5`: ISO8859-5, Latin/Cyrillic
	* `iso8859-6`: ISO8859-6, Latin/Arabic
	* `iso8859-7`: ISO8859-7, Latin/Greek
	* `iso8859-8`: ISO8859-8, Latin/Hebrew
	* `iso8859-9`: ISO8859-9, Latin-5
	* `iso8859-10`: ISO8859-10, Latin-6
	* `iso8859-13`: ISO8859-13, Latin-7
	* `iso8859-14`: ISO8859-14, Latin-8
	* `iso8859-15`: ISO8859-15, Latin-9
	* `iso8859-16`: ISO8859-16, Latin-10
	* `cp437`: IBM CodePage 437
	* `cp850`: IBM CodePage 850
	* `cp852`: IBM CodePage 852
	* `cp855`: IBM CodePage 855
	* `cp858`: IBM CodePage 858
	* `cp860`: IBM CodePage 860
	* `cp862`: IBM CodePage 862
	* `cp863`: IBM CodePage 863
	* `cp865`: IBM CodePage 865
	* `cp866`: IBM CodePage 866
	* `ebcdic-037`: IBM CodePage 037
	* `ebcdic-1040`: IBM CodePage 1140
	* `ebcdic-1047`: IBM CodePage 1047
	* `koi8r`: KOI8-R, Russian (Cyrillic)
	* `koi8u`: KOI8-U, Ukranian (Cyrillic)
	* `macintosh`: Macintosh encoding
	* `macintosh-cyrillic`: Macintosh Cyrillic encoding
	* `windows1250`: Windows1250, Central and Eastern European
	* `windows1251`: Windows1251, Russian, Serbian (Cyrillic)
	* `windows1252`: Windows1252, Legacy
	* `windows1253`: Windows1253, Modern Greek
	* `windows1254`: Windows1254, Turkish
	* `windows1255`: Windows1255, Hebrew
	* `windows1256`: Windows1256, Arabic
	* `windows1257`: Windows1257, Estonian, Latvian, Lithuanian
	* `windows1258`: Windows1258, Vietnamese
	* `windows874`:  Windows874, ISO/IEC 8859-11, Latin/Thai
	* `utf-16-bom`: UTF-16 with required BOM
	* `utf-16be-bom`: big endian UTF-16 with required BOM
	* `utf-16le-bom`: little endian UTF-16 with required BOM

The `plain` encoding is special, because it does not validate or transform any
input.
// end::encoding-setting[]

// =============================================================================

// tag::exclude_files-setting[]

// QUESTION: Will prospector.scanner.exclude_files replace this setting
// completely, or just in the filestream input?
|
[id="input-{input-type}-exclude_files-setting"]
`exclude_files`

| (list) A list of regular expressions to match the files that you want
{agent} to ignore.

*Default:* no files excluded

The following example configures {agent} to ignore all files that have a `gz`
extension:

[source,yaml,subs="attributes"]
----
inputs:
  - type: {input-type}
    ...
    exclude_files: ['\.gz$']
----

//See <<regexp-support>> for a list of supported regexp patterns.

// end::exclude_files-setting[]

// =============================================================================

// tag::exclude_lines-setting[]
|
[id="input-{input-type}-exclude_lines-setting"]
`exclude_lines`

| (list) A list of regular expressions to match the lines you want to
exclude. {agent} drops any lines that match a regular expression in the
list. Empty lines are ignored.

*Default:* no lines are excluded

//If <<multiline,multiline>> settings are also specified, each multiline message
//is combined into a single line before the lines are filtered by `exclude_lines`.

The following example configures {agent} to drop any lines that start with
`DBG`.

["source","yaml",subs="attributes"]
----
inputs:
  - type: {input-type}
    ...
    exclude_lines: ['^DBG']
----

//See <<regexp-support>> for a list of supported regexp patterns. 
// end::exclude_lines-setting[]

// =============================================================================

// tag::fields-setting[]
|
[id="input-{input-type}-fields-setting"]
`fields`

| Optional fields to add to the output. You can add fields to use for
filtering data. Fields can be scalar values, arrays, dictionaries, or any
nested combination. By default, the fields are grouped under a `fields`
sub-dictionary in the output. To create top-level fields, set
`fields_under_root` to `true`. If a duplicate field is declared in the general
configuration, its value will be overwritten by the value declared here.

Example:

["source","yaml",subs="attributes"]
-----
inputs:
  - type: {input-type}
    . . .
    fields:
      app_id: query_engine_12
-----
// end::fields-setting[]

// =============================================================================

// tag::fields-under-root-setting[]
|
[id="input-{input-type}-fields-under-root-setting"]
`fields_under_root`

| (boolean) If `true`, custom fields are stored as top-level fields in the
output document instead of being grouped under a `fields` sub-dictionary. If the
custom field names conflict with other field names, the custom fields overwrite
the other fields.

*Default:* `false`
// end::fields-under-root-setting[]

// =============================================================================

// tag::file_identity-setting[]
|
[id="input-{input-type}-file_identity-setting"]
`file_identity`

| Specify `file_identity` methods to suit the environment where you are
collecting log messages.

*Default*: `file_identity.native: ~`

*`file_identity.native`*:: Identify files based on their inodes and device ids.
+
--
[source,yaml]
----
file_identity.native: ~
----

--

*`file_identity.path`*:: Identify files based on their paths.
+
--
IMPORTANT: Only use this strategy if your log files are rotated to a folder
outside of the scope of your input or not at all. Otherwise you end up
with duplicated events.

IMPORTANT: This strategy does not support renaming files.
If an input file is renamed, {agent} will read it again if the new path
matches the settings of the input.

[source,yaml]
----
file_identity.path: ~
----

--

*`file_identity.inode_marker`*:: If the device ID changes from time to time, you
must use this method to distinguish files. This option is not supported on
Windows.
+
--
Set the location of the marker file the following way:

[source,yaml]
----
file_identity.inode_marker:
  path: /logs/.filebeat-marker
----

--
// end::file_identity-setting[]

// =============================================================================

// tag::harvester_limit-setting[]
|
[id="input-{input-type}-harvester_limit-setting"]
`harvester_limit`

| (int) Limits the number of harvesters that are started in parallel for one
input. This directly relates to the maximum number of file handlers that are
opened.

*Default:* `0` (no limit)

Change this setting if the number of files to be harvested exceeds the open file
handler limit of the operating system.

Setting a limit on the number of harvesters means that potentially not all files
are opened in parallel. Therefore we recommended that you use this option in
combination with the `close_*` options to make sure harvesters are stopped more
often so that new files can be picked up.

Currently if a new harvester can be started again, the harvester is picked
randomly. This means it's possible that the harvester for a file that was just
closed and then updated again might be started instead of the harvester for a
file that hasn't been harvested for a longer period of time.

This configuration option applies per input. You can use this option to
indirectly set higher priorities on certain inputs by assigning a higher limit
of harvesters.
// end::harvester_limit-setting[]

// =============================================================================

// tag::ignore_older-setting[]
|
[id="input-{input-type}-ignore_older-setting"]
`ignore_older`

| (string) If `true`, {agent} ignores any files modified before the specified
timespan. This setting is useful if you keep log files for a long time and only
want to send newer files.

*Default:* `0` (no files ignored)

You can use time strings like 2h (2 hours) and 5m (5 minutes). 0 disables the
setting. Commenting out the config has the same effect as setting it to 0.

IMPORTANT: You must set `ignore_older` to be greater than `close_inactive`.

The files affected by this setting fall into two categories:

* Files that were never harvested
* Files that were harvested but weren't updated for longer than `ignore_older`

For files that were never seen before, the offset state is set to the end of
the file. If a state already exist, the offset is not changed. In case a file is
updated again later, reading continues at the set offset position.

The `ignore_older` setting relies on the modification time of the file to
determine if a file is ignored. If the modification time of the file is not
updated when lines are written to a file (which can happen on Windows), the
`ignore_older` setting may cause {agent} to ignore files even though
content was added at a later time.

To remove the state of previously harvested files from the registry file, use
the `clean_inactive` configuration option.

Before a file can be ignored by {agent}, the file must be closed. To
ensure a file is no longer being harvested when it is ignored, you must set
`ignore_older` to a longer duration than `close_inactive`.

If a file that's currently being harvested falls under `ignore_older`, the
harvester will first finish reading the file and close it after `close_inactive`
is reached. Then, after that, the file will be ignored.

// end::ignore_older-setting[]

// =============================================================================

// tag::include_lines-setting[]
|
[id="input-{input-type}-include_lines-setting"]
`include_lines`

| (list) A list of regular expressions to match the lines you want to
include. {agent} exports only the lines that match a regular expression in
the list. Empty lines are ignored.

*Default:* all lines are exported

//Add after multiline doc is added. Note that filestream does not yet support multiline.
//If <<multiline,multiline>> settings also specified, each multiline message is
//combined into a single line before the lines are filtered by `include_lines`.

The following example configures {agent} to export any lines that start
with `ERR` or `WARN`:

["source","yaml",subs="attributes"]
----
inputs:
  - type: {input-type}
    ...
    include_lines: ['^ERR', '^WARN']
----

NOTE: If both `include_lines` and `exclude_lines` are defined, {agent}
executes `include_lines` first and then executes `exclude_lines`. The order in
which the two options are defined doesn't matter. The `include_lines` option
will always be executed before the `exclude_lines` option, even if
`exclude_lines` appears before `include_lines` in the config file.

The following example exports all log lines that contain `sometext`,
except for lines that begin with `DBG` (debug messages):

["source","yaml",subs="attributes"]
----
inputs:
  - type: {input-type}
    ...
    include_lines: ['sometext']
    exclude_lines: ['^DBG']
----

//See <<regexp-support>> for a list of supported regexp patterns.

// end::include_lines-setting[]

// =============================================================================

// tag::harvester_buffer_size-setting[]
|
[id="input-{input-type}-harvester_buffer_size-setting"]
`harvester_buffer_size`

| (int) The size in bytes of the buffer that each harvester uses when fetching
a file.

*Default:* `16384`
// end::harvester_buffer_size-setting[]

// =============================================================================

// tag::index-setting[]
// TODO: Remove this???
|
[id="input-{input-type}-index"]
`index`

| (string) If present, this formatted string overrides the index for events from
this input (for {es} outputs), or sets the `raw_index` field of the event's
metadata (for other outputs). This string can only refer to the agent name and
version and the event timestamp; for access to dynamic fields, use
`output.elasticsearch.index` or a processor

["source","yaml",subs="attributes"]
-----
inputs:
  - type: {input-type}
    . . .
    index: "%{[agent.name]}-myindex-%{+yyyy.MM.dd}"
-----

The index in the example might expand to `"filebeat-myindex-2019.11.01"`.
// end::index-setting[]

// =============================================================================

// tag::json-setting[]
//QUESTION: Should we create a separate section for JSON decoding?
|
[id="input-{input-type}-json-setting"]
`json`

| These options make it possible for {agent} to decode logs structured as
JSON messages. {agent} processes the logs line by line, so the JSON
decoding only works if there is one JSON object per line.

The decoding happens before line filtering and multiline. You can combine JSON
decoding with filtering and multiline if you set the `message_key` option. This
can be helpful in situations where the application logs are wrapped in JSON
objects, as with Docker logs.

Example configuration:

[source,yaml]
----
json.keys_under_root: true
json.add_error_key: true
json.message_key: log
----

You must specify at least one of the following settings to enable JSON parsing
mode:

`json.keys_under_root`:: By default, the decoded JSON is placed under a `json`
key in the output document. If `true`, the keys are copied to the top level in
the output document. Default: `false`

`json.overwrite_keys`:: If `keys_under_root` and this setting are `true`, then the
values from the decoded JSON object overwrite the fields that {agent}
normally adds (type, source, offset, and so on) in case of conflicts.

`json.expand_keys`:: If `true`, {agent} will recursively de-dot keys in the decoded
JSON, and expand them into a hierarchical object structure. For example,
`{"a.b.c": 123}` is expanded into `{"a":{"b":{"c":123}}}`. Set this option to
`true` when the input is produced by an
https://github.com/elastic/ecs-logging[ECS logger].

`json.add_error_key`:: If `true`, {agent} adds `error.message` and
`error.type: json` keys in case of JSON unmarshalling errors
or when a `message_key` is defined in the configuration but cannot be used.

`json.message_key`:: Specifies a JSON key on which to apply the line filtering and
multiline settings. If specified, the key must be at the top level in the JSON
object, and the value associated with the key must be a string, otherwise no
filtering or multiline aggregation will occur.

`json.document_id`:: Specifies the JSON key to set the document id. If configured,
the field will be removed from the original JSON document and stored in
`@metadata._id`

`json.ignore_decoding_error`:: Specifies whether JSON decoding errors should be
logged. If `true`, errors will not be logged. Default: `false`
// end::json-setting[]

// =============================================================================

// tag::keep_null-setting[]
|
[id="input-{input-type}-keep_null-setting"]
`keep_null`

| (boolean) If `true`, fields with `null` values are published in the output
document.

*Default:* `false`
// end::keep_null-setting[]

// =============================================================================

// tag::max_backoff-setting[]
|
[id="input-{input-type}-max_backoff-setting"]
`max_backoff`

| (string) The maximum time for {agent} to wait before checking a file again
after EOF is reached. After having backed off multiple times from checking the
file, the wait time will never exceed `max_backoff` regardless of what is
specified for `backoff_factor`.

*Default:* `10s`

Because it takes a maximum of 10s to read a new line, specifying `10s` for
`max_backoff` means that, at the worst, a new line could be added to the log
file if {agent} has backed off multiple times.

Requirement: Set `max_backoff` to be greater than or equal to `backoff` and less
than or equal to `scan_frequency` (`backoff <= max_backoff <= scan_frequency`).
If `max_backoff` needs to be higher, you should close the file handler instead
and let {agent} pick up the file again.
// end::max_backoff-setting[]

// =============================================================================

// tag::max_bytes-setting[]
|
[id="input-{input-type}-max_bytes-setting"]
`max_bytes`

| (int) The maximum number of bytes that a single log message can have. All
bytes after `max_bytes` are discarded and not sent. This setting is especially
useful for multiline log messages, which can get large.

*Default:* `10485760` (10MB)
// end::max_bytes-setting[]

// =============================================================================

// tag::multiline-setting[]
|
[id="input-{input-type}-multiline-setting"]
`multiline`

| Options that control how {agent} deals with log messages that span
multiple lines.

//See <<multiline-examples>> for more information about configuring multiline options.
// end::multiline-setting[]

// =============================================================================

// tag::paths-setting[]
|
[id="input-{input-type}-paths-setting"]
`paths`

| (list) A list of glob-based paths that will be crawled and fetched. All
patterns supported by https://golang.org/pkg/path/filepath/#Glob[Go Glob] are
also supported here. For example, to fetch all files from a predefined level of
subdirectories, use the following pattern: `/var/log/*/*.log`. This fetches all
`.log` files from the subfolders of `/var/log`. It does not fetch log files from
the `/var/log` folder itself.

{agent} starts a harvester for each file that it finds under the specified
paths. You can specify one path per line. Each line begins with a dash (-).
// end::paths-setting[]

// =============================================================================

// tag::pipeline-setting[]
|
[id="input-{input-type}-pipeline-setting"]
`pipeline`

| (string) The Ingest Node pipeline ID to set for the events generated by this
input.

NOTE: The pipeline ID can also be configured in the {es} output, but this option
usually results in simpler configuration files. If the pipeline is configured
both in the input and output, the option from the input is used.

// end::pipeline-setting[]

// =============================================================================

// tag::processors-setting[]
|
[id="input-{input-type}-processors-setting"]
`processors`

| (list) A list of processors to apply to the input data.

//See <<filtering-and-enhancing-data>> for information about specifying
//processors in your config.
// end::processors-setting[]

// =============================================================================

// tag::publisher_pipeline.disable_host-setting[]
|
[id="input-{input-type}-publisher_pipeline.disable_host-setting"]
`publisher_pipeline`
`.disable_host`

| (boolean) Removes `host.name` from all events.

*Default:* `false`
// end::publisher_pipeline.disable_host-setting[]

// =============================================================================

// tag::scan_frequency-setting[]
|
[id="input-{input-type}-scan_frequency-setting"]
`scan_frequency`

| (string) How often {agent} checks for new files in the paths that are specified
for harvesting.

*Default:* `10s`

For example, if you specify a glob like `/var/log/*`, the directory is scanned
for files using the frequency specified by `scan_frequency`. Specify `1s` to scan
the directory as frequently as possible without causing {agent} to scan too
frequently. We do not recommend setting this value to less than `1s`.

If you require log lines to be sent in near real time do not use a very low
`scan_frequency` but adjust `close_inactive` so the file handler stays open and
constantly polls your files.
// end::scan_frequency-setting[]

// =============================================================================

// tag::scan.order-setting[]
|
[id="input-{input-type}-scan.order-setting"]
`scan.order`

|
experimental[]

(string) Specifies whether to use ascending or descending order when `scan.sort`
is set. Possible values are `asc` or `desc`.

*Default:* `asc`
// end::scan.order-setting[]

// =============================================================================

// tag::scan.sort-setting[]
|
[id="input-{input-type}-scan.sort-setting"]
`scan.sort`

| 
experimental[]

(string) Specifies how to sort files when they're scanned. Possible values are
`modtime`, which sorts by file modification time, and `filename`. Specify an
empty string to turn off this setting.

*Default:* `""` (no sorting)

If you specify a value for this setting, use `scan.order` to configure whether
files are scanned in ascending or descending order.
// end::scan.sort-setting[]

// =============================================================================

// tag::ssl-setting[]
|
[id="input-{input-type}-ssl-setting"]
`ssl`

| Configuration options for SSL parameters like the certificate, key, and the
certificate authorities to use.

//See <<configuration-ssl>> for more information.

// end::ssl-setting[]

// =============================================================================

// tag::symlinks-setting[]
|
[id="input-{input-type}-symlinks-setting"]
`symlinks`

| (boolean) If `true`, {agent} harvests symlinks in addition to regular files.
When harvesting symlinks, the agent opens and reads the original file even
though it reports the path of the symlink.

*Default:* `false`

When you configure a symlink for harvesting, make sure the original path is
excluded. If a single input is configured to harvest both the symlink and
the original file, {agent} detects the problem and only process the
first file it finds. However, if two different inputs are configured (one
to read the symlink and the other the original path), both paths are
harvested, causing {agent} to send duplicate data and the inputs to
overwrite each other's state.

The `symlinks` option can be useful if symlinks to the log files have additional
metadata in the file name, and you want to process the metadata in {ls}.
This is, for example, the case for Kubernetes log files.

Because this option may lead to data loss, it is `false` by default.
// end::symlinks-setting[]

// =============================================================================

// tag::tags-setting[]
|
[id="input-{input-type}-tags-setting"]
`tags`

| (list) A list of tags to include in the `tags` field of each published event.
Tags make it easy to select specific events in {kib} or apply conditional
filtering in {ls}. These tags are appended to the list of tags specified in the
general configuration.

Example:

[source,yaml]
-----
inputs:
  - type: {input-type}
    . . .
    tags: ["json"]
-----

// end::tags-setting[]

// =============================================================================

// tag::tail_files-setting[]
|
[id="input-{input-type}-tail_files-setting"]
`tail_files`

| (boolean) If `true`, {agent} starts reading new files at the end of each file
instead of the beginning. When this setting is used in combination with log
rotation, the first log entries in a new file might be skipped. 

*Default:* `false`

This option applies to files that {agent} has not already processed. If you ran
{agent} previously and the state of the file was already persisted, `tail_files`
does not apply. Harvesting continues at the previous offset. To apply
`tail_files` to all files, you must stop {agent} and remove the registry file.
Be aware that doing this removes ALL previous states.

NOTE: Use this setting to avoid indexing old log lines when you run {agent} on a
set of log files for the first time. After the first run, we recommend setting
this option back to `false`, or you risk losing lines during file rotation.

// end::tail_files-setting[]

// =============================================================================

// tag::tcp-framing-setting[]
|
[id="input-{input-type}-tcp-framing-setting"]
`framing`

| (string) The framing used to split incoming events.  Can be one of
`delimiter` or `rfc6587`. `delimiter` uses the characters specified
in `line_delimiter` to split the incoming events. `rfc6587` supports
octet counting and non-transparent framing as described in
https://tools.ietf.org/html/rfc6587[RFC6587]. `line_delimiter` is
used to split the events in non-transparent framing.

*Default:* `delimiter`
// end::tcp-framing-setting[]

// =============================================================================

// tag::tcp-host-setting[]
|
[id="input-{input-type}-tcp-host-setting"]
`host`

| (string) The host and TCP port to listen on for event streams.
// end::tcp-host-setting[]

// =============================================================================

// tag::tcp-line_delimiter-setting[]
|
[id="input-{input-type}-tcp-line_delimiter-setting"]
`line_delimiter`

| (string) The characters used to split the incoming events.

*Default:* `'\n'`
// end::tcp-line_delimiter-setting[]

// =============================================================================

// tag::tcp-max_connections-setting[]
|
[id="input-{input-type}-tcp-max_connections-setting"]
`max_connections`

| (int) The maximum number of connections to accept at any time.

*Default:* `????`

//QUESTION: What is the default?

// end::tcp-max_connections-setting[]

// =============================================================================

// tag::tcp-max_message_size-setting[]
|
[id="input-{input-type}-tcp-max_message_size-setting"]
`max_message_size`

| (string) The maximum size of the message received over TCP.

*Default:* `20MiB`
// end::tcp-max_message_size-setting[]

// =============================================================================

// tag::tcp-timeout-setting[]
|
[id="input-{input-type}-tcp-timeout-setting"]
`timeout`

| (string) The number of seconds of inactivity before a remote connection is
closed.

*Default:* `300s`

// end::tcp-timeout-setting[]

// =============================================================================

// tag::udp-host-setting[]
|
[id="input-{input-type}-udp-host-setting"]
`host`

| (string) The host and UDP port to listen on for event streams.

// end::udp-host-setting[]

// =============================================================================

// tag::udp-max_message_size-setting[]
|
[id="input-{input-type}-udp-max_message_size-setting"]
`max_message_size`

| (string) The maximum size of the message received over UDP.

*Default:* `10KiB`

// end::udp-max_message_size-setting[]

// =============================================================================

// tag::udp-read_buffer-setting[]
|
[id="input-{input-type}-udp-read_buffer-setting"]
`read_buffer`

| (string) The size of the read buffer on the UDP socket.

*Default:* `???`

//QUESTION: What is the default?

// end::udp-read_buffer-setting[]

// =============================================================================

// tag::udp-timeout-setting[]
|
[id="input-{input-type}-udp-timeout-setting"]
`timeout`

| (string) The read and write timeout for socket operations.

*Default:* `????`

//QUESTION: What is the default?

// end::udp-timeout-setting[]

// =============================================================================

// tag::unix-framing-setting[]
|
[id="input-{input-type}-unix-framing-setting"]
`framing`

| (string) The framing used to split incoming events.  Can be one of
`delimiter` or `rfc6587`. `delimiter` uses the characters specified
in `line_delimiter` to split the incoming events. `rfc6587` supports
octet counting and non-transparent framing as described in
https://tools.ietf.org/html/rfc6587[RFC6587]. `line_delimiter` is
used to split the events in non-transparent framing.

*Default:* `delimiter`
// end::unix-framing-setting[]

// =============================================================================

// tag::unix-group-setting[]
|
[id="input-{input-type}-unix-group-setting"]
`group`

| (string) The group ownership of the Unix socket that will be created by
{agent}.

*Default:* the primary group name for the user {agent} is running
 
This option is ignored on Windows.
// end::unix-group-setting[]

// =============================================================================

// tag::unix-line_delimiter-setting[]
|
[id="input-{input-type}-unix-line_delimiter-setting"]
`line_delimiter`

| (string) The characters used to split the incoming events.

*Default:* `'\n'`
// end::unix-line_delimiter-setting[]

// =============================================================================

// tag::unix-max_connections-setting[]
|
[id="input-{input-type}-unix-max_connections-setting"]
`max_connections`

| (int) The maximum number of connections to accept at any time.

*Default:* `????`

//QUESTION: What is the default?

// end::unix-max_connections-setting[]

// =============================================================================

// tag::unix-max_message_size-setting[]
|
[id="input-{input-type}-unix-max_message_size-setting"]
`max_message_size`

| (string) The maximum size of the message received over the socket. 

*Default:* `20MiB`
// end::unix-max_message_size-setting[]

// =============================================================================

// tag::unix-mode-setting[]
|
[id="input-{input-type}-unix-mode-setting"]
`mode`

| (string) The file mode of the Unix socket that will be created by {agent}.
This is expected to be a file mode as an octal string.

*Default:* system default (usually `0755`)
// end::unix-mode-setting[]

// =============================================================================

// tag::unix-path-setting[]
|
[id="input-{input-type}-unix-path-setting"]
`path`

| (string) The path to the Unix socket that will receive events.
// end::unix-path-setting[]

// =============================================================================

// tag::unix-socket_type-setting[]
|
[id="input-{input-type}-unix-socket_type-setting"]
`socket_type`

| (string) The type to of the Unix socket that will receive events. Valid values
are `stream` and `datagram`.

*Default:* `stream`
// end::unix-socket_type-setting[]

// =============================================================================

// tag::unix-timeout-setting[]
|
[id="input-{input-type}-unix-timeout-setting"]
`timeout`

The number of seconds of inactivity before a connection is closed. The default
is `300s`.

// end::unix-timeout-setting[]

// =============================================================================

